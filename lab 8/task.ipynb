{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jjAJaNgXyr8",
        "outputId": "b918b983-b14c-4573-fff9-5a76c5a6ce07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Loss=3.9041, k=1.4683, b=1.4642\n",
            "Epoch 200: Loss=3.9000, k=1.6547, b=1.3624\n",
            "Epoch 300: Loss=3.9001, k=1.7126, b=1.3511\n",
            "Epoch 400: Loss=3.8999, k=1.6776, b=1.3554\n",
            "Epoch 500: Loss=3.9001, k=1.6408, b=1.3771\n",
            "Epoch 600: Loss=3.8999, k=1.6770, b=1.3631\n",
            "Epoch 700: Loss=3.9000, k=1.6564, b=1.3662\n",
            "Epoch 800: Loss=3.9002, k=1.7110, b=1.3558\n",
            "Epoch 900: Loss=3.9000, k=1.7114, b=1.3500\n",
            "Epoch 1000: Loss=3.8999, k=1.7081, b=1.3399\n",
            "Epoch 1100: Loss=3.9002, k=1.6604, b=1.3793\n",
            "Epoch 1200: Loss=3.8999, k=1.7041, b=1.3371\n",
            "Epoch 1300: Loss=3.8999, k=1.6727, b=1.3543\n",
            "Epoch 1400: Loss=3.9002, k=1.7379, b=1.3364\n",
            "Epoch 1500: Loss=3.9001, k=1.7003, b=1.3299\n",
            "Epoch 1600: Loss=3.9002, k=1.7080, b=1.3244\n",
            "Epoch 1700: Loss=3.8999, k=1.6989, b=1.3453\n",
            "Epoch 1800: Loss=3.9002, k=1.6893, b=1.3679\n",
            "Epoch 1900: Loss=3.9006, k=1.6774, b=1.3288\n",
            "Epoch 2000: Loss=3.9003, k=1.6822, b=1.3330\n",
            "Epoch 2100: Loss=3.9005, k=1.6677, b=1.3356\n",
            "Epoch 2200: Loss=3.9010, k=1.6998, b=1.3786\n",
            "Epoch 2300: Loss=3.8999, k=1.7022, b=1.3477\n",
            "Epoch 2400: Loss=3.9000, k=1.6898, b=1.3371\n",
            "Epoch 2500: Loss=3.9002, k=1.6940, b=1.3308\n",
            "Epoch 2600: Loss=3.9000, k=1.6817, b=1.3437\n",
            "Epoch 2700: Loss=3.9000, k=1.6929, b=1.3392\n",
            "Epoch 2800: Loss=3.8999, k=1.7018, b=1.3516\n",
            "Epoch 2900: Loss=3.9003, k=1.7154, b=1.3562\n",
            "Epoch 3000: Loss=3.8999, k=1.7187, b=1.3331\n",
            "Epoch 3100: Loss=3.9000, k=1.6907, b=1.3384\n",
            "Epoch 3200: Loss=3.8999, k=1.6796, b=1.3600\n",
            "Epoch 3300: Loss=3.9006, k=1.6753, b=1.3835\n",
            "Epoch 3400: Loss=3.9000, k=1.6774, b=1.3460\n",
            "Epoch 3500: Loss=3.9002, k=1.7096, b=1.3571\n",
            "Epoch 3600: Loss=3.9000, k=1.7244, b=1.3330\n",
            "Epoch 3700: Loss=3.8999, k=1.6681, b=1.3576\n",
            "Epoch 3800: Loss=3.9009, k=1.6685, b=1.3287\n",
            "Epoch 3900: Loss=3.9001, k=1.7260, b=1.3406\n",
            "Epoch 4000: Loss=3.8999, k=1.6855, b=1.3531\n",
            "Epoch 4100: Loss=3.8999, k=1.7013, b=1.3507\n",
            "Epoch 4200: Loss=3.9016, k=1.6600, b=1.3241\n",
            "Epoch 4300: Loss=3.9010, k=1.7268, b=1.3635\n",
            "Epoch 4400: Loss=3.8999, k=1.6775, b=1.3498\n",
            "Epoch 4500: Loss=3.9006, k=1.6487, b=1.3454\n",
            "Epoch 4600: Loss=3.9002, k=1.6737, b=1.3416\n",
            "Epoch 4700: Loss=3.9005, k=1.6851, b=1.3272\n",
            "Epoch 4800: Loss=3.9016, k=1.6901, b=1.3919\n",
            "Epoch 4900: Loss=3.9015, k=1.6989, b=1.3855\n",
            "Epoch 5000: Loss=3.9007, k=1.7300, b=1.3572\n",
            "Epoch 5100: Loss=3.9008, k=1.7267, b=1.3608\n",
            "Epoch 5200: Loss=3.9003, k=1.6739, b=1.3388\n",
            "Epoch 5300: Loss=3.9001, k=1.7138, b=1.3501\n",
            "Epoch 5400: Loss=3.9014, k=1.7046, b=1.3042\n",
            "Epoch 5500: Loss=3.9015, k=1.6880, b=1.3106\n",
            "Epoch 5600: Loss=3.8999, k=1.6957, b=1.3475\n",
            "Epoch 5700: Loss=3.9022, k=1.7014, b=1.3926\n",
            "Epoch 5800: Loss=3.8999, k=1.6965, b=1.3544\n",
            "Epoch 5900: Loss=3.9000, k=1.6902, b=1.3630\n",
            "Epoch 6000: Loss=3.8999, k=1.6914, b=1.3454\n",
            "Epoch 6100: Loss=3.9000, k=1.7040, b=1.3348\n",
            "Epoch 6200: Loss=3.9002, k=1.6940, b=1.3665\n",
            "Epoch 6300: Loss=3.9000, k=1.7124, b=1.3460\n",
            "Epoch 6400: Loss=3.9001, k=1.6799, b=1.3394\n",
            "Epoch 6500: Loss=3.9002, k=1.6917, b=1.3298\n",
            "Epoch 6600: Loss=3.9001, k=1.7233, b=1.3446\n",
            "Epoch 6700: Loss=3.9000, k=1.6880, b=1.3611\n",
            "Epoch 6800: Loss=3.9002, k=1.6759, b=1.3397\n",
            "Epoch 6900: Loss=3.8999, k=1.6784, b=1.3498\n",
            "Epoch 7000: Loss=3.9004, k=1.6690, b=1.3384\n",
            "Epoch 7100: Loss=3.9001, k=1.7027, b=1.3572\n",
            "Epoch 7200: Loss=3.9001, k=1.6828, b=1.3380\n",
            "Epoch 7300: Loss=3.8999, k=1.7048, b=1.3415\n",
            "Epoch 7400: Loss=3.9013, k=1.6999, b=1.3068\n",
            "Epoch 7500: Loss=3.9001, k=1.6890, b=1.3342\n",
            "Epoch 7600: Loss=3.9001, k=1.6881, b=1.3356\n",
            "Epoch 7700: Loss=3.9001, k=1.6994, b=1.3594\n",
            "Epoch 7800: Loss=3.9006, k=1.6768, b=1.3301\n",
            "Epoch 7900: Loss=3.8999, k=1.6948, b=1.3535\n",
            "Epoch 8000: Loss=3.8999, k=1.6964, b=1.3461\n",
            "Epoch 8100: Loss=3.8999, k=1.6864, b=1.3595\n",
            "Epoch 8200: Loss=3.9000, k=1.6772, b=1.3443\n",
            "Epoch 8300: Loss=3.9001, k=1.7234, b=1.3435\n",
            "Epoch 8400: Loss=3.9000, k=1.6892, b=1.3632\n",
            "Epoch 8500: Loss=3.9044, k=1.7495, b=1.3854\n",
            "Epoch 8600: Loss=3.8999, k=1.6840, b=1.3500\n",
            "Epoch 8700: Loss=3.8999, k=1.7083, b=1.3404\n",
            "Epoch 8800: Loss=3.9002, k=1.6819, b=1.3722\n",
            "Epoch 8900: Loss=3.9006, k=1.6842, b=1.3797\n",
            "Epoch 9000: Loss=3.9005, k=1.6792, b=1.3295\n",
            "Epoch 9100: Loss=3.8999, k=1.6763, b=1.3554\n",
            "Epoch 9200: Loss=3.9005, k=1.6605, b=1.3410\n",
            "Epoch 9300: Loss=3.8999, k=1.6931, b=1.3475\n",
            "Epoch 9400: Loss=3.9002, k=1.6789, b=1.3361\n",
            "Epoch 9500: Loss=3.9007, k=1.6678, b=1.3332\n",
            "Epoch 9600: Loss=3.8999, k=1.6829, b=1.3474\n",
            "Epoch 9700: Loss=3.9005, k=1.7205, b=1.3587\n",
            "Epoch 9800: Loss=3.9001, k=1.7328, b=1.3367\n",
            "Epoch 9900: Loss=3.9000, k=1.6731, b=1.3665\n",
            "Epoch 10000: Loss=3.9003, k=1.6521, b=1.3511\n",
            "Epoch 10100: Loss=3.9002, k=1.6665, b=1.3450\n",
            "Epoch 10200: Loss=3.9005, k=1.6774, b=1.3312\n",
            "Epoch 10300: Loss=3.8999, k=1.6991, b=1.3379\n",
            "Epoch 10400: Loss=3.9000, k=1.6668, b=1.3533\n",
            "Epoch 10500: Loss=3.9006, k=1.6870, b=1.3776\n",
            "Epoch 10600: Loss=3.9006, k=1.6425, b=1.3497\n",
            "Epoch 10700: Loss=3.9000, k=1.7173, b=1.3292\n",
            "Epoch 10800: Loss=3.8999, k=1.6980, b=1.3494\n",
            "Epoch 10900: Loss=3.9001, k=1.6942, b=1.3312\n",
            "Epoch 11000: Loss=3.9000, k=1.6617, b=1.3577\n",
            "Epoch 11100: Loss=3.9003, k=1.7043, b=1.3618\n",
            "Epoch 11200: Loss=3.9012, k=1.7205, b=1.3704\n",
            "Epoch 11300: Loss=3.9000, k=1.6662, b=1.3675\n",
            "Epoch 11400: Loss=3.9004, k=1.6554, b=1.3865\n",
            "Epoch 11500: Loss=3.9019, k=1.6662, b=1.3168\n",
            "Epoch 11600: Loss=3.8999, k=1.6784, b=1.3520\n",
            "Epoch 11700: Loss=3.9001, k=1.7168, b=1.3478\n",
            "Epoch 11800: Loss=3.8999, k=1.6841, b=1.3530\n",
            "Epoch 11900: Loss=3.9008, k=1.7252, b=1.3614\n",
            "Epoch 12000: Loss=3.8999, k=1.7022, b=1.3486\n",
            "Epoch 12100: Loss=3.9011, k=1.7043, b=1.3774\n",
            "Epoch 12200: Loss=3.9000, k=1.6797, b=1.3440\n",
            "Epoch 12300: Loss=3.9000, k=1.6799, b=1.3645\n",
            "Epoch 12400: Loss=3.8999, k=1.6800, b=1.3507\n",
            "Epoch 12500: Loss=3.9001, k=1.6761, b=1.3418\n",
            "Epoch 12600: Loss=3.9000, k=1.6792, b=1.3650\n",
            "Epoch 12700: Loss=3.8999, k=1.6951, b=1.3552\n",
            "Epoch 12800: Loss=3.9002, k=1.6880, b=1.3682\n",
            "Epoch 12900: Loss=3.9001, k=1.7146, b=1.3497\n",
            "Epoch 13000: Loss=3.9002, k=1.6513, b=1.3551\n",
            "Epoch 13100: Loss=3.9007, k=1.6974, b=1.3750\n",
            "Epoch 13200: Loss=3.9013, k=1.6931, b=1.3103\n",
            "Epoch 13300: Loss=3.9005, k=1.6904, b=1.3251\n",
            "Epoch 13400: Loss=3.8999, k=1.7030, b=1.3458\n",
            "Epoch 13500: Loss=3.9001, k=1.6598, b=1.3523\n",
            "Epoch 13600: Loss=3.9011, k=1.7193, b=1.3690\n",
            "Epoch 13700: Loss=3.8999, k=1.6633, b=1.3625\n",
            "Epoch 13800: Loss=3.9007, k=1.6604, b=1.3370\n",
            "Epoch 13900: Loss=3.9009, k=1.6697, b=1.3279\n",
            "Epoch 14000: Loss=3.9002, k=1.6758, b=1.3381\n",
            "Epoch 14100: Loss=3.9002, k=1.6926, b=1.3663\n",
            "Epoch 14200: Loss=3.8999, k=1.6784, b=1.3608\n",
            "Epoch 14300: Loss=3.9001, k=1.6689, b=1.3450\n",
            "Epoch 14400: Loss=3.9001, k=1.6802, b=1.3394\n",
            "Epoch 14500: Loss=3.9005, k=1.7245, b=1.3091\n",
            "Epoch 14600: Loss=3.9000, k=1.7027, b=1.3344\n",
            "Epoch 14700: Loss=3.9001, k=1.7081, b=1.3551\n",
            "Epoch 14800: Loss=3.9016, k=1.6782, b=1.3138\n",
            "Epoch 14900: Loss=3.9011, k=1.7188, b=1.3696\n",
            "Epoch 15000: Loss=3.8999, k=1.6971, b=1.3524\n",
            "Epoch 15100: Loss=3.9004, k=1.6999, b=1.3225\n",
            "Epoch 15200: Loss=3.8999, k=1.7049, b=1.3388\n",
            "Epoch 15300: Loss=3.9008, k=1.6960, b=1.3156\n",
            "Epoch 15400: Loss=3.9003, k=1.6901, b=1.3291\n",
            "Epoch 15500: Loss=3.9015, k=1.6825, b=1.3137\n",
            "Epoch 15600: Loss=3.9005, k=1.6881, b=1.3253\n",
            "Epoch 15700: Loss=3.9000, k=1.7301, b=1.3350\n",
            "Epoch 15800: Loss=3.9029, k=1.7076, b=1.3957\n",
            "Epoch 15900: Loss=3.9005, k=1.6952, b=1.3227\n",
            "Epoch 16000: Loss=3.9002, k=1.6915, b=1.3309\n",
            "Epoch 16100: Loss=3.9000, k=1.7010, b=1.3531\n",
            "Epoch 16200: Loss=3.9000, k=1.6865, b=1.3404\n",
            "Epoch 16300: Loss=3.8999, k=1.6999, b=1.3456\n",
            "Epoch 16400: Loss=3.9000, k=1.6773, b=1.3455\n",
            "Epoch 16500: Loss=3.8999, k=1.7001, b=1.3387\n",
            "Epoch 16600: Loss=3.9003, k=1.6963, b=1.3251\n",
            "Epoch 16700: Loss=3.9000, k=1.7196, b=1.3311\n",
            "Epoch 16800: Loss=3.9005, k=1.6484, b=1.3489\n",
            "Epoch 16900: Loss=3.9028, k=1.6814, b=1.4076\n",
            "Epoch 17000: Loss=3.8999, k=1.6787, b=1.3473\n",
            "Epoch 17100: Loss=3.9016, k=1.7289, b=1.3712\n",
            "Epoch 17200: Loss=3.9005, k=1.6990, b=1.3196\n",
            "Epoch 17300: Loss=3.8999, k=1.6858, b=1.3508\n",
            "Epoch 17400: Loss=3.8999, k=1.7012, b=1.3479\n",
            "Epoch 17500: Loss=3.9012, k=1.6933, b=1.3124\n",
            "Epoch 17600: Loss=3.9000, k=1.7077, b=1.3534\n",
            "Epoch 17700: Loss=3.8999, k=1.7068, b=1.3465\n",
            "Epoch 17800: Loss=3.9002, k=1.7000, b=1.3264\n",
            "Epoch 17900: Loss=3.9018, k=1.6723, b=1.3147\n",
            "Epoch 18000: Loss=3.9006, k=1.6923, b=1.3763\n",
            "Epoch 18100: Loss=3.9000, k=1.6875, b=1.3416\n",
            "Epoch 18200: Loss=3.9003, k=1.6858, b=1.3313\n",
            "Epoch 18300: Loss=3.9000, k=1.6816, b=1.3423\n",
            "Epoch 18400: Loss=3.9000, k=1.6991, b=1.3368\n",
            "Epoch 18500: Loss=3.9001, k=1.6822, b=1.3391\n",
            "Epoch 18600: Loss=3.8999, k=1.6898, b=1.3467\n",
            "Epoch 18700: Loss=3.9001, k=1.7155, b=1.3259\n",
            "Epoch 18800: Loss=3.9001, k=1.7067, b=1.3562\n",
            "Epoch 18900: Loss=3.9006, k=1.7137, b=1.3637\n",
            "Epoch 19000: Loss=3.9004, k=1.7120, b=1.3177\n",
            "Epoch 19100: Loss=3.8999, k=1.6882, b=1.3587\n",
            "Epoch 19200: Loss=3.8999, k=1.6944, b=1.3512\n",
            "Epoch 19300: Loss=3.9004, k=1.6645, b=1.3851\n",
            "Epoch 19400: Loss=3.8999, k=1.6816, b=1.3564\n",
            "Epoch 19500: Loss=3.9000, k=1.6951, b=1.3576\n",
            "Epoch 19600: Loss=3.9001, k=1.6775, b=1.3423\n",
            "Epoch 19700: Loss=3.8999, k=1.6875, b=1.3490\n",
            "Epoch 19800: Loss=3.9001, k=1.7070, b=1.3549\n",
            "Epoch 19900: Loss=3.8999, k=1.7017, b=1.3502\n",
            "Epoch 20000: Loss=3.8999, k=1.6931, b=1.3522\n",
            "\n",
            "Training completed!\n",
            "Final parameters: k=1.6931, b=1.3522\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1. Data generation\n",
        "np.random.seed(42)  # For reproducibility\n",
        "x_data = np.random.uniform(0, 1, 1000).reshape(-1, 1)\n",
        "y_data = 2 * x_data + 1 + np.random.normal(0, 2, size=(1000, 1))\n",
        "\n",
        "# 2. Creating data as tensors\n",
        "X = tf.convert_to_tensor(x_data, dtype=tf.float32)\n",
        "y = tf.convert_to_tensor(y_data, dtype=tf.float32)\n",
        "\n",
        "# 3. Initializing model variables\n",
        "k = tf.Variable(tf.random.normal([1], stddev=0.1), name='k')\n",
        "b = tf.Variable(tf.zeros([1]), name='b')\n",
        "\n",
        "# 4. Building the linear regression model\n",
        "def model(X):\n",
        "    return k * X + b\n",
        "\n",
        "# 5. Defining the loss function and optimizer\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# 6. Training the model\n",
        "num_epochs = 20000\n",
        "batch_size = 100\n",
        "num_batches = len(x_data) // batch_size\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(num_batches):\n",
        "        # Selecting a random mini-batch\n",
        "        batch_indices = np.random.choice(len(x_data), batch_size)\n",
        "        x_batch = tf.convert_to_tensor(x_data[batch_indices], dtype=tf.float32)\n",
        "        y_batch = tf.convert_to_tensor(y_data[batch_indices], dtype=tf.float32)\n",
        "\n",
        "        # Optimization within GradientTape\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(x_batch)\n",
        "            loss = compute_loss(y_batch, y_pred)\n",
        "\n",
        "        gradients = tape.gradient(loss, [k, b])\n",
        "        optimizer.apply_gradients(zip(gradients, [k, b]))\n",
        "\n",
        "    # Printing results every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        y_pred_full = model(X)\n",
        "        full_loss = compute_loss(y, y_pred_full).numpy()\n",
        "        print(f\"Epoch {epoch + 1}: Loss={full_loss:.4f}, k={k.numpy()[0]:.4f}, b={b.numpy()[0]:.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Final parameters: k={k.numpy()[0]:.4f}, b={b.numpy()[0]:.4f}\")\n"
      ]
    }
  ]
}